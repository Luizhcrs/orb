# ORB Backend

Backend Python para o assistente ORB - Assistente de IA flutuante.

## ğŸš€ CaracterÃ­sticas

- **FastAPI**: Framework web moderno e rÃ¡pido
- **WebSocket**: ComunicaÃ§Ã£o em tempo real
- **LLM Integration**: Suporte para OpenAI e Anthropic
- **Tool Selector**: Sistema flexÃ­vel para ferramentas futuras
- **Screenshot**: Captura de tela integrada
- **Hot Corner**: DetecÃ§Ã£o de canto quente (planejado)

## ğŸ“ Estrutura do Projeto

```
backend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agentes/
â”‚   â”‚   â””â”€â”€ orb_agent/
â”‚   â”‚       â”œâ”€â”€ agente.py          # Pipeline principal do agente
â”‚   â”‚       â”œâ”€â”€ llms/
â”‚   â”‚       â”‚   â””â”€â”€ llm_provider.py # Provedores LLM (OpenAI, Anthropic)
â”‚   â”‚       â”œâ”€â”€ tools/
â”‚   â”‚       â”‚   â””â”€â”€ tool_selector.py # Seletor de ferramentas
â”‚   â”‚       â”œâ”€â”€ prompts/
â”‚   â”‚       â”‚   â””â”€â”€ system_prompt.yaml # Prompt do sistema
â”‚   â”‚       â””â”€â”€ utils/
â”‚   â”‚           â””â”€â”€ logging_config.py # ConfiguraÃ§Ã£o de logging
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ main.py               # AplicaÃ§Ã£o FastAPI principal
â”‚   â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”‚   â””â”€â”€ api_config.py     # ConfiguraÃ§Ãµes da API
â”‚   â”‚   â””â”€â”€ routers/
â”‚   â”‚       â”œâ”€â”€ agent.py          # Endpoints do agente
â”‚   â”‚       â”œâ”€â”€ health.py         # Health checks
â”‚   â”‚       â”œâ”€â”€ system.py         # Funcionalidades do sistema
â”‚   â”‚       â””â”€â”€ websocket.py      # WebSocket em tempo real
â”‚   â”œâ”€â”€ services/                 # ServiÃ§os do sistema
â”‚   â”œâ”€â”€ models/                   # Modelos de dados
â”‚   â””â”€â”€ utils/                    # UtilitÃ¡rios
â”œâ”€â”€ tests/                        # Testes
â”œâ”€â”€ scripts/                      # Scripts auxiliares
â”œâ”€â”€ docs/                         # DocumentaÃ§Ã£o
â”œâ”€â”€ main.py                       # Ponto de entrada
â”œâ”€â”€ requirements.txt              # DependÃªncias Python
â””â”€â”€ env.example                   # Exemplo de variÃ¡veis de ambiente
```

## ğŸ› ï¸ InstalaÃ§Ã£o

1. **Clone o repositÃ³rio e navegue para o backend:**
   ```bash
   cd backend
   ```

2. **Crie um ambiente virtual:**
   ```bash
   python -m venv venv
   source venv/bin/activate  # Linux/Mac
   # ou
   venv\Scripts\activate     # Windows
   ```

3. **Instale as dependÃªncias:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure as variÃ¡veis de ambiente:**
   ```bash
   cp env.example .env
   # Edite o arquivo .env com suas chaves de API
   ```

## âš™ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

Crie um arquivo `.env` baseado no `env.example`:

```env
# LLM Providers
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# ConfiguraÃ§Ãµes do modelo
DEFAULT_MODEL=gpt-3.5-turbo
MAX_TOKENS=1000
TEMPERATURE=0.7

# ConfiguraÃ§Ãµes do servidor
HOST=0.0.0.0
PORT=8000
DEBUG=true

# CORS settings
CORS_ORIGINS=http://localhost:3000,http://localhost:8080
```

### Chaves de API

Configure pelo menos uma das chaves de API:

- **OpenAI**: Obtenha em https://platform.openai.com/api-keys
- **Anthropic**: Obtenha em https://console.anthropic.com/

## ğŸš€ ExecuÃ§Ã£o

### Desenvolvimento

```bash
python main.py
```

O servidor estarÃ¡ disponÃ­vel em:
- **API**: http://localhost:8000
- **DocumentaÃ§Ã£o**: http://localhost:8000/docs
- **WebSocket**: ws://localhost:8000/ws

### ServiÃ§o Windows (Recomendado para ProduÃ§Ã£o)

```bash
# Instalar como serviÃ§o Windows
python scripts/install_service.py

# Iniciar serviÃ§o
python scripts/start_service.py

# Gerenciar serviÃ§o
python scripts/service_manager.py
```

**Vantagens do ServiÃ§o Windows:**
- âœ… Inicia automaticamente com o Windows
- âœ… ExecuÃ§Ã£o em background (sem interface)
- âœ… Restart automÃ¡tico em caso de falha
- âœ… Logs centralizados no Event Viewer
- âœ… Melhor integraÃ§Ã£o com sistema Windows

ğŸ“– **DocumentaÃ§Ã£o completa**: [docs/WINDOWS_SERVICE.md](docs/WINDOWS_SERVICE.md)

### ProduÃ§Ã£o (Alternativa)

```bash
uvicorn src.api.main:app --host 0.0.0.0 --port 8000
```

## ğŸ“¡ API Endpoints

### Health Check
- `GET /health/` - Health check bÃ¡sico
- `GET /health/detailed` - Health check detalhado
- `GET /health/ready` - Readiness check
- `GET /health/live` - Liveness check

### Agente
- `POST /agent/message` - Enviar mensagem para o agente
- `GET /agent/status` - Status do agente
- `POST /agent/reset` - Resetar contexto de sessÃ£o
- `GET /agent/sessions` - SessÃµes ativas
- `DELETE /agent/sessions/{session_id}` - Remover sessÃ£o

### Sistema
- `POST /system/screenshot` - Capturar screenshot
- `GET /system/status` - Status do sistema
- `POST /system/hot-corner/configure` - Configurar hot corner
- `POST /system/orb/toggle` - Alternar orb
- `GET /system/orb/status` - Status do orb

### WebSocket
- `WS /ws` - ConexÃ£o WebSocket em tempo real

## ğŸ”Œ WebSocket

### Tipos de Mensagem

#### Enviar para o servidor:
```json
{
  "type": "message",
  "message": "OlÃ¡, como vocÃª pode me ajudar?",
  "session_id": "uuid-opcional",
  "image_data": "base64-opcional"
}
```

#### Receber do servidor:
```json
{
  "type": "response",
  "content": "OlÃ¡! Como posso ajudÃ¡-lo hoje?",
  "session_id": "uuid",
  "model_used": "gpt-3.5-turbo",
  "provider": "openai",
  "timestamp": "2025-01-05T20:00:00"
}
```

### Outros tipos de mensagem:
- `ping/pong` - Heartbeat
- `processing` - Indicador de processamento
- `error` - Erro
- `connection` - ConfirmaÃ§Ã£o de conexÃ£o

## ğŸ§ª Testes

```bash
# Executar todos os testes
pytest

# Executar com coverage
pytest --cov=src

# Executar testes especÃ­ficos
pytest tests/test_agent.py
```

## ğŸ—ï¸ Arquitetura

### Pipeline do Agente

1. **Recebe mensagem** â†’ Valida session_id
2. **Verifica contexto** â†’ Recupera histÃ³rico da conversa
3. **Seleciona tools** â†’ Decide se precisa de ferramentas especÃ­ficas
4. **Gera resposta** â†’ Usa LLM para gerar resposta
5. **Salva contexto** â†’ Armazena conversa no histÃ³rico

### Componentes

- **AgenteORB**: Pipeline principal do agente
- **LLMProvider**: Gerenciador de provedores LLM
- **ToolSelector**: Seletor inteligente de ferramentas
- **ConnectionManager**: Gerenciador de conexÃµes WebSocket

## ğŸ”® Roadmap

- [ ] IntegraÃ§Ã£o com banco de dados para persistÃªncia
- [ ] ImplementaÃ§Ã£o de hot corner
- [ ] Controle do orb flutuante
- [ ] Ferramentas especÃ­ficas (RAG, cÃ¡lculos, etc.)
- [ ] AutenticaÃ§Ã£o e autorizaÃ§Ã£o
- [ ] MÃ©tricas e monitoramento
- [ ] Testes automatizados completos

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

## ğŸ†˜ Suporte

Para suporte, abra uma issue no repositÃ³rio ou entre em contato com a equipe.
